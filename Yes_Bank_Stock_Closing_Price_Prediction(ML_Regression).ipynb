{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "q29F0dvdveiT",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohitwagishorg/Ml-capstone-project/blob/main/Yes_Bank_Stock_Closing_Price_Prediction(ML_Regression).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Regression\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member 1 -** Mohit wagish\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the summary here within 500-600 words."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write Problem Statement Here.**"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "# Importing Libraries\n",
        "import numpy as np               # For numerical operations on arrays and matrices\n",
        "import pandas as pd              # For data manipulation and analysis\n",
        "import matplotlib.pyplot as plt  # For basic data visualization\n",
        "import seaborn as sns            # For statistical data visualization\n",
        "plt.style.use('ggplot')          # Setting the plot style to 'ggplot'\n",
        "import plotly.express as px      # For creating various interactive visualizations\n",
        "\n",
        "%matplotlib inline\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "LMXe1PeaNaKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stock_df=pd.read_csv('/content/drive/MyDrive/data_YesBank_StockPrices.csv')"
      ],
      "metadata": {
        "id": "mx0rqreFODpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "stock_df.sample(5)"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "stock_df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "stock_df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "stock_df.duplicated().value_counts()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "stock_df.isna().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "# Generate heatmap of missing values\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(stock_df.isnull(), cmap='viridis',annot=True)\n",
        "plt.title('Missing Values Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is no missing value"
      ],
      "metadata": {
        "id": "Is7Ed6Ba7AJ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Open: The price at which a stock began trading when the market opened on a particular day.\n",
        "\n",
        "High: The highest price reached by a stock during a specific period.\n",
        "\n",
        "Low: The lowest price reached by a stock during a specific period.\n",
        "\n",
        "Date: The date corresponding to the stock price data.\n",
        "\n",
        "Close: The price of an individual stock when the stock exchange closed its market for the day. (Target/dependent variable)"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "list(stock_df.columns)"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "stock_df.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset encompasses monthly records of Yes Bank stock prices since its debut on the stock exchange. It includes vital features such as:\n",
        "\n",
        "- **Date:** Signifying the specific month under observation.\n",
        "- **Open:** Reflecting the stock's price at the inception of the trading day.\n",
        "- **High:** Denoting the peak price achieved by the stock within the month.\n",
        "- **Low:** Representing the minimum price recorded by the stock during the month.\n",
        "- **Close:** Indicating the stock's price at the conclusion of the trading day.\n",
        "\n",
        "This dataset presents a panoramic view of Yes Bank's monthly performance, detailing its opening, highest, lowest, and closing prices since its inception on the stock exchange.Answer Here"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "stock_df.nunique()\n"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stock_df.columns"
      ],
      "metadata": {
        "id": "vSbDWrwWgOnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "# Changing date colunn datatype to datetime format.\n",
        "from datetime import datetime\n",
        "\n",
        "# parsing date which is string of format %b-%y to datetime (%b for Month as localeâ€™s abbreviated name and %y for Year without century as a zero-padded decimal number.\n",
        "stock_df['Date'] = stock_df['Date'].apply(lambda x: datetime.strptime(x, '%b-%y'))"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Extracting Year from Date column\n",
        "stock_df['Year']=stock_df['Date'].dt.year\n",
        "#Extracting Month from Date column\n",
        "stock_df['Month']=stock_df['Date'].dt.month\n",
        "#Extracting quarter from Date column\n",
        "stock_df['Quarter']=stock_df['Date'].dt.quarter"
      ],
      "metadata": {
        "id": "8_skhXjF9y0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define numerical columns\n",
        "num_cols = stock_df.select_dtypes(include='number').columns\n",
        "\n",
        "# Plot box plots to check for outliers\n",
        "fig, axes = plt.subplots(nrows=4, ncols=2, figsize=(14, 10), constrained_layout=True)\n",
        "fig.suptitle('Box Plots for Numerical Columns', fontsize=16, color='navy')\n",
        "\n",
        "# Adjusting subplot parameters\n",
        "fig.subplots_adjust(left=0.05, bottom=0.08, right=0.95, top=0.9, wspace=0.3, hspace=0.4)\n",
        "\n",
        "# Define colors for box plots\n",
        "colors = ['skyblue', 'salmon', 'lightgreen', 'gold', 'orchid', 'lightcoral', 'lightseagreen', 'lightskyblue']\n",
        "\n",
        "# Iterating over each subplot\n",
        "for ax, column, color in zip(axes.flatten(), num_cols, colors):\n",
        "    # Plotting box plot for the current column\n",
        "    sns.boxplot(x=stock_df[column], ax=ax, color=color, orient='h', linewidth=1.5)  # Using orient='h' for horizontal boxplot\n",
        "    ax.set_title(column, fontsize=12, color='darkblue')  # Setting subplot title\n",
        "    ax.set_xlabel('Values', fontsize=10, color='darkgreen')\n",
        "    ax.set_ylabel('')  # No need for y-axis label for horizontal boxplot\n",
        "    ax.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MgIu6LIK98R1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a separate DataFrame to store outliers\n",
        "outliers_df = pd.DataFrame(columns=stock_df.columns)\n",
        "\n",
        "# Find outliers for each column\n",
        "for column in stock_df.columns:\n",
        "    Q1 = stock_df[column].quantile(0.25)\n",
        "    Q3 = stock_df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    outliers = stock_df[(stock_df[column] < (Q1 - 1.5 * IQR)) | (stock_df[column] > (Q3 + 1.5 * IQR))]\n",
        "    outliers_df = pd.concat([outliers_df, outliers], ignore_index=True)\n",
        "\n",
        "# Display DataFrame containing outliers\n",
        "print(\"DataFrame containing outliers:\")\n",
        "print(outliers_df)"
      ],
      "metadata": {
        "id": "u1L-CgqPA165"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stock_df.describe()"
      ],
      "metadata": {
        "id": "QTCfDJ2AA5JL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####1. After transforming the 'Date' column from string format to a date object, we introduced a new column named 'Year' to capture the individual years extracted from each date entry.\n",
        "\n",
        "####2.Moreover, from the 'Date' column, we derived 'Month' and 'Quarter' variables to provide further insights into the temporal distribution of the data.\n",
        "\n",
        "####3.In the process of outlier analysis, it was observed that the dataset exhibited no outliers, indicating the robustness of the data distribution.\n",
        "\n",
        "####4.Furthermore, a thorough examination confirmed the absence of null values within the dataset, thus affirming its integrity and completeness.\n",
        "\n",
        "####5.The box plots vividly reveal outliers across all four columns, which have been subsequently extracted and stored in a separate dataframe. However, given the inherent volatility in stock prices, we refrain from categorizing these outliers as irrelevant. Instead, recognizing their potential utility, we retain them within our dataset for further analysis."
      ],
      "metadata": {
        "id": "UEZMS6sLB1E-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Create a Figure object with Candlestick chart\n",
        "fig = go.Figure(go.Candlestick(\n",
        "    x=stock_df.index,            # x-axis values (dates)\n",
        "    open=stock_df['Open'],       # open prices\n",
        "    high=stock_df['High'],       # high prices\n",
        "    low=stock_df['Low'],         # low prices\n",
        "    close=stock_df['Close']      # close prices\n",
        "))\n",
        "\n",
        "# Update the layout of the figure with a title\n",
        "fig.update_layout(\n",
        "    title={'text': 'Describing Price Movements', 'x': 0.5, 'y': 0.95, 'font': {'color': 'white', 'size': 20}},\n",
        "    xaxis=dict(title='Year', title_font={'color': 'white', 'size': 16}, tickfont={'color': 'white', 'size': 12}),\n",
        "    yaxis=dict(title='Price', title_font={'color': 'white', 'size': 16}, tickfont={'color': 'white', 'size': 12}),\n",
        "    width=1200,\n",
        "    height=800,\n",
        "    plot_bgcolor='rgb(36, 40, 47)',  # Set the background color to a professional dark gray\n",
        "    paper_bgcolor='rgb(51, 56, 66)'  # Set the paper color\n",
        ")\n",
        "\n",
        "# Show the figure\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer - The specific chart chosen in the provided code snippet is a Candlestick chart. Candlestick charts are commonly used in financial analysis to visualize the movement of stock prices over time. Each candlestick represents the open, high, low, and close prices for a specific period (e.g., day, week, month), providing valuable insights into price trends, volatility, and market sentiment.\n",
        "\n",
        "Here's why the Candlestick chart was chosen:\n",
        "\n",
        "1. **Relevance to Financial Data:** Candlestick charts are specifically designed to represent financial data, making them suitable for analyzing stock prices.\n",
        "\n",
        "2. **Comprehensive Representation:** Each candlestick encapsulates four essential price points (open, high, low, close), offering a comprehensive view of price movements within a given period.\n",
        "\n",
        "3. **Easy Interpretation:** Candlestick patterns are intuitive to interpret, allowing analysts to quickly identify trends, reversals, and potential trading opportunities.\n",
        "\n",
        "4. **Visual Appeal:** Candlestick charts are visually appealing and can effectively communicate complex price dynamics to stakeholders.\n",
        "\n",
        "Given that the goal is to describe price movements in the dataset, the Candlestick chart is a suitable choice due to its ability to provide a detailed and insightful representation of stock price data."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer - The analysis of Yes Bank's stock prices uncovers a distinctive narrative. Before 2018, the stock showcased a steadfast upward trajectory, signaling buoyant growth and reflecting a positive outlook among investors. However, a notable downturn ensued thereafter, chiefly attributed to the Yes Bank fraud case implicating Rana Kapoor, its former CEO.\n",
        "\n",
        "In the lead-up to 2018, the stock enjoyed a sustained ascent, underscoring favorable market sentiments and investor faith. Yet, the disclosure of the fraud case involving Rana Kapoor marked a pivotal juncture, precipitating a sharp decline in stock prices.\n",
        "\n",
        "The ramifications of the fraud case reverberated profoundly, significantly dampening investor confidence and trust in Yes Bank. Consequently, the stock witnessed a palpable depreciation, mirroring the adverse impact of the scandal on the company's reputation and financial well-being.\n",
        "\n",
        "In essence, the analysis underscores the divergent trajectories in Yes Bank's stock prices. Pre-2018, there was a consistent upward trend, whereas the post-2018 period saw a marked downturn due to the reverberations of the fraud case involving Rana Kapoor."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer - Insights from analyzing Yes Bank's stock prices can positively impact decision-making by informing strategies to navigate market volatility and rebuild investor confidence. However, the revelation of the fraud case involving Rana Kapoor damaged the bank's reputation, potentially leading to negative growth due to loss of trust and financial instability. Proactive measures to address these challenges are crucial for sustaining positive business impact and fostering growth."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2 plotting 'High' Variable"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.distplot(stock_df['High'], color='red')\n",
        "plt.title(\"Distribution\", fontsize=18)\n",
        "plt.xlabel('High', fontsize=12)\n",
        "plt.ylabel('Frequency', fontsize=12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer- To see the distribution and hence apply necessary operation to make it normally distributed as for regression problem normally distributed performs best"
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6,5))\n",
        "sns.distplot(np.log(stock_df['High']), color='pink')\n",
        "plt.title(\"Distribution\", fontsize=16)\n",
        "plt.xlabel('High', fontsize=12)\n",
        "plt.ylabel('Frequency', fontsize=12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "I0PXyg83CXGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer - A right-skewed distribution in stock price history suggests that lower prices are more common than higher prices, indicating potential for occasional significant gains but also higher volatility and risk. Investors should be cautious and conduct thorough analysis before making investment decisions."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Ans - Yes, insights from the right-skewed distribution can help tailor investment strategies, manage risks, make informed decisions, and communicate effectively with investors, leading to positive business impact in finance.\n",
        "#### Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "Ans - Overconfidence in potential gains, underestimation of risks, lack of diversification, and ignoring fundamental analysis can lead to negative growth despite insights from a right-skewed distribution of stock prices."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3 Distribution of 'Open' Variable"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.distplot(stock_df['Open'], color='yellow')\n",
        "plt.title(\"Distribution\", fontsize=16)\n",
        "plt.xlabel('Open', fontsize=12)\n",
        "plt.ylabel('Frequency', fontsize=12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer - To see the distribution of open of my stock, to understand the history of my stock in one sort."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer - The insights from the right-skewed distribution of the stock's opening graph suggest higher occurrences of lower opening prices, occasional significant gains, volatility, and long-term growth potential, but also highlight the need to manage associated risks."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer - The gained insights from the right-skewed distribution of the stock's opening graph can potentially create a positive business impact by informing investment strategies to capitalize on occasional significant gains and manage risks effectively. However, failure to adequately manage risks, such as volatility and overreliance on short-term gains, could lead to negative growth. Overemphasis on speculative trading based solely on occasional spikes in opening prices without considering fundamental factors may result in unsustainable growth and potential losses in the long term. Therefore, while the insights offer opportunities, careful risk management and consideration of long-term fundamentals are crucial to sustain positive growth."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4 Distribution of 'Low' Variable"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.distplot(stock_df['Low'], color='green')\n",
        "plt.title(\"Distribution\", fontsize=18)\n",
        "plt.xlabel('Low', fontsize=12)\n",
        "plt.ylabel('Frequency', fontsize=12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer - To achieve a thorough understanding of the distribution, employing a distplot accompanied by a kernel density estimation (KDE) curve proves to be the most effective visualization technique. This method presents a holistic view of the feature's data points, enabling a deeper comprehension of its distribution traits."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?\n"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer - Insights from the right-skewed distribution of the stock's low prices indicate more frequent lower lows, occasional significant increases, volatility, and potential long-term growth with caution."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer - The gained insights from the right-skewed distribution of the stock's low prices can potentially create a positive business impact by informing investment strategies to capitalize on occasional significant increases and manage risks effectively. However, failure to adequately manage risks, such as volatility and overreliance on short-term gains, could lead to negative growth. Overemphasis on speculative trading based solely on occasional spikes in low prices without considering fundamental factors may result in unsustainable growth and potential losses in the long term. Therefore, while the insights offer opportunities, careful risk management and consideration of long-term fundamentals are crucial to sustain positive growth."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5 Distribution of 'Close' Variable"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.distplot(stock_df['Close'], color='blue')\n",
        "plt.title(\"Distribution\", fontsize=16)\n",
        "plt.xlabel('Close', fontsize=12)\n",
        "plt.ylabel('Frequency', fontsize=12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer - to see the distribution and usderstand the behaviour"
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer - The right-skewed distribution of the stock's closing prices suggests more occurrences of lower prices, occasional significant increases, volatility, and long-term growth potential, highlighting both opportunities for gains and the need for risk management."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer - The insights gained from the right-skewed distribution of the stock's closing prices can potentially create a positive business impact by informing investment strategies to capitalize on occasional significant increases and manage risks effectively. However, failure to adequately manage risks, such as volatility and overreliance on short-term gains, could lead to negative growth. Overemphasis on speculative trading based solely on occasional spikes in closing prices without considering fundamental factors may result in unsustainable growth and potential losses in the long term. Therefore, while the insights offer opportunities, careful risk management and consideration of long-term fundamentals are crucial to sustain positive growth."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6 Seasonal Decomposition"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization codez\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "result = seasonal_decompose(stock_df['Close'], model='additive', period=12)\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.subplot(411)\n",
        "plt.plot(result.observed)\n",
        "plt.title('Observed')\n",
        "plt.subplot(412)\n",
        "plt.plot(result.trend)\n",
        "plt.title('Trend')\n",
        "plt.subplot(413)\n",
        "plt.plot(result.seasonal)\n",
        "plt.title('Seasonal')\n",
        "plt.subplot(414)\n",
        "plt.plot(result.resid)\n",
        "plt.title('Residual')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer - The selected visualization is the seasonal decomposition plot, which breaks down the closing prices of Yes Bank stock into four parts: observed, trend, seasonal, and residual. This plot has been selected for analyzing the underlying trends in the data across time, including any seasonal patterns that may exist."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer - Observed: Displays the initial Yes Bank stock closing prices.\n",
        "\n",
        "Trend: Illustrates the overarching, long-term direction of the stock prices, minimizing short-term variations.\n",
        "\n",
        "Seasonal: Exhibits the recurring pattern or fluctuations happening at consistent intervals throughout the year.\n",
        "\n",
        "Residual: Represents the unpredictable or irregular segment of the data, not accounted for by the trend or seasonal elements."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer - Certainly, here's a concise summary in bullet points:\n",
        "\n",
        "**Positive Business Impact:**\n",
        "1. Informed Decision-Making: Insights from components aid in strategic decisions like investments and pricing.\n",
        "2. Risk Management: Understanding residual component helps in mitigating unpredictable risks.\n",
        "3. Performance Evaluation: Trend analysis facilitates assessment against long-term objectives.\n",
        "\n",
        "**Negative Growth Potential:**\n",
        "1. Misinterpretation of Seasonal Patterns: Wrong interpretation of fluctuations may lead to incorrect decisions.\n",
        "2. Failure to Adapt: Ignoring emerging trends identified may result in failure to adapt, leading to negative growth.\n",
        "3. Overlooking Residual Risks: Neglecting residual risks can leave businesses vulnerable to uncertainties, potentially resulting in financial losses."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7 scatter plot to see the relationship between dependent & independent variables"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "\n",
        "for col in stock_df.describe().columns[:-1]:\n",
        "  fig = plt.figure(figsize=(20,5))\n",
        "  ax = fig.gca()\n",
        "  plt.scatter(stock_df[col], stock_df['Close'])\n",
        "  plt.xlabel(col)\n",
        "  plt.ylabel('Close')\n",
        "  ax.set_title('{} vs Close'.format(col))\n",
        "  z = np.polyfit(stock_df[col], stock_df['Close'], 1)\n",
        "  y_hat = np.poly1d(z)(stock_df[col])\n",
        "  plt.plot(stock_df[col], y_hat, \"r--\", lw=1)\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer - To observe whether the dependent variables and independent variables are linealy dependent or not"
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer - we can see that close variable are lineraly dependent on high,open variable"
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "correlation_matrix = stock_df[['Open', 'High', 'Low', 'Close']].corr()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Matrix of Open, High, Low, Close Prices')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilizing a heatmap of the correlation matrix in the stock_df DataFrame enables visualization of variable relationships. Annotated correlation coefficients offer quantitative insights, enhanced by color scheme customization. The heatmap swiftly identifies strong correlations and interdependencies, aiding in portfolio diversification and risk management strategies. It serves as a valuable tool for decision-making in stock market analysis and investment strategies."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The independent variables exhibit strong correlations among themselves, while the dependent variable, Close, shows high correlation with Open, High, and Low.\n",
        "\n",
        "These insights from correlation analysis can inform investment decisions and trading strategies. For instance:\n",
        "\n",
        "- Traders utilizing range-bound strategies can benefit from the strong correlation between High and Low prices.\n",
        "- Understanding the relationship between Open, High, Low, and Close prices can aid in predicting future price movements and identifying trade entry or exit points."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "# Create a pair plot to explore relationships in the stock_df DataFrame\n",
        "sns.pairplot(stock_df)\n",
        "\n",
        "# Set the background colors for the figure and axes\n",
        "plot_bgcolor = (36/255, 40/255, 47/255, 1)  # RGB values divided by 255, with alpha=1 for full opacity\n",
        "paper_bgcolor = (51/255, 56/255, 66/255, 1)\n",
        "\n",
        "# Customize the appearance of the figure and axes\n",
        "plt.gcf().set_facecolor(plot_bgcolor)  # Set the background color of the figure\n",
        "plt.gca().set_facecolor(paper_bgcolor)  # Set the background color of the axes\n",
        "\n",
        "# Display the pair plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer - Opting for a pairs plot was a deliberate choice as it offers a comprehensive view of both individual variable distributions and pairwise relationships within the dataset. This visualization method enables swift detection of patterns and facilitates the identification of potential trends and areas warranting deeper exploration."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer - The interplay among the variables Open, High, Low, and Close unveils intriguing insights into the dynamics of Yes Bank stock. Notably, there exists a robust correlation between these variables, particularly between Open, High, Low, and Close. This suggests a close relationship among the stock's opening, highest, lowest, and closing prices.\n",
        "\n",
        "Furthermore, Open, High, and Low demonstrate a striking correlation among themselves, indicating synchronized movements and shared trends. These correlations serve as pivotal indicators for analyzing Yes Bank stock, potentially serving as predictors of future closing prices.\n",
        "\n",
        "This intricate web of relationships underscores the interconnected nature of the stock market, hinting at the influence of external factors on stock performance. Leveraging this understanding empowers stakeholders to make informed decisions and discern patterns for forecasting future price movements.\n",
        "\n",
        "However, it's essential to approach these correlations with caution, as correlation does not imply causation. A comprehensive analysis requires a holistic consideration of additional factors and nuanced insights to truly grasp the complexities of stock market behavior."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer -\n",
        "#### Null Hypothesis (H0): There is no significant difference in the mean closing price between different quarters.\n",
        "#### Alternative Hypothesis (H1): There is a significant difference in the mean closing price between at least one pair of quarters."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Extract closing prices for each quarter\n",
        "quarterly_closing_prices = [stock_df[stock_df['Quarter'] == q]['Close'] for q in range(1, 5)]\n",
        "\n",
        "# One-way ANOVA test\n",
        "statistic, p_value = stats.f_oneway(*quarterly_closing_prices)\n",
        "\n",
        "# Interpret results\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Reject null hypothesis: There is a significant difference in the mean closing price between quarters.\")\n",
        "else:\n",
        "    print(\"Fail to reject null hypothesis: There is no significant difference in the mean closing price between quarters.\")\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer -  One-way ANOVA or Kruskal-Wallis test to compare the means of closing prices across different quarters."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer - The choice between one-way ANOVA and the Kruskal-Wallis test depends on data assumptions and measurement level. One-way ANOVA assumes normality and equal variances, suitable for interval data. Kruskal-Wallis is non-parametric, suitable for non-normal data or unequal variances, and applicable to ordinal, interval, or ratio data. Choose ANOVA if assumptions are met; otherwise, use Kruskal-Wallis for robustness."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer -\n",
        "Null Hypothesis (H0): The time series has a unit root, indicating it is non-stationary.\n",
        "\n",
        "Alternative Hypothesis (H1): The time series does not have a unit root, indicating it is stationary."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "# Perform ADF test\n",
        "result = adfuller(stock_df['Close'])\n",
        "\n",
        "# Extract test statistic and p-value\n",
        "test_statistic = result[0]\n",
        "p_value = result[1]\n",
        "\n",
        "# Define significance level\n",
        "alpha = 0.05\n",
        "\n",
        "# Interpret results\n",
        "if p_value < alpha:\n",
        "    print(\"Reject null hypothesis: The time series is stationary.\")\n",
        "else:\n",
        "    print(\"Fail to reject null hypothesis: The time series is non-stationary.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer - The statistical test performed to obtain the p-value in the code example is the Augmented Dickey-Fuller (ADF) test. The ADF test is a hypothesis test used to determine whether a unit root is present in a time series dataset, indicating its stationarity. The p-value obtained from the ADF test is then compared to a chosen significance level (e.g., 0.05) to make conclusions about the stationarity of the time series."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer - The Augmented Dickey-Fuller (ADF) test was chosen because it is specifically designed to assess stationarity in time series data. It is widely used, applicable to various types of data, provides clear interpretation through p-values, and is readily available in statistical libraries like statsmodels."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer - Null Hypothesis (H0): There is no significant difference in the mean closing price across different months.\n",
        "\n",
        "Alternative Hypothesis (H1): There is a significant difference in the mean closing price across at least one pair of months."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# Extract closing prices for each month\n",
        "monthly_closing_prices = [stock_df[stock_df['Month'] == m]['Close'] for m in range(1, 13)]\n",
        "\n",
        "# Kruskal-Wallis test\n",
        "statistic, p_value = stats.kruskal(*monthly_closing_prices)\n",
        "\n",
        "# Interpret results\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Reject null hypothesis: There is a significant difference in the mean closing price across different months.\")\n",
        "else:\n",
        "    print(\"Fail to reject null hypothesis: There is no significant difference in the mean closing price across different months.\")\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer - One-way ANOVA or Kruskal-Wallis test to compare the means of closing prices across different months."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer - If the normality assumption is violated or if the variances across months are not equal, the Kruskal-Wallis test is a robust alternative. It does not rely on distributional assumptions and is effective for comparing medians across groups."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "stock_df.isna().sum()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer - The dataset does't contain the missing values."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "'''\n",
        "No Outliers found\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "'NOt required'"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Before all columns distribution"
      ],
      "metadata": {
        "id": "lr09iKUrE8n-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "cols=['Open','High','Low']\n",
        "for i in cols:\n",
        "  plt.figure(figsize=(6,5))\n",
        "  sns.distplot(stock_df[i], color='green')\n",
        "  plt.title(\"Distribution\", fontsize=16)\n",
        "  plt.xlabel(i, fontsize=12)\n",
        "  plt.ylabel('Frequency', fontsize=12)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can observe that all columns are right skewed and to make it normally distributed we can apply log transform"
      ],
      "metadata": {
        "id": "Bd33ZLGRFI9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cols=['Open','High','Low']\n",
        "for i in cols:\n",
        "  plt.figure(figsize=(6,5))\n",
        "  sns.distplot(np.log(stock_df[i]), color='blue')\n",
        "  plt.title(\"Distribution\", fontsize=16)\n",
        "  plt.xlabel(i, fontsize=12)\n",
        "  plt.ylabel('Frequency', fontsize=12)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "9vMaDvDoFVHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the set of  independent and dependent variables\n",
        "X = stock_df.drop(labels=['Close','Date'], axis=1)\n",
        "\n",
        "y = stock_df['Close']"
      ],
      "metadata": {
        "id": "cdvzgVWyGS2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "x = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "ZqitN-fIGpkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "StandardScaler is chosen for data scaling because it is robust to outliers, preserves the shape of the original distribution, ensures compatibility with algorithms sensitive to feature scales, facilitates interpretation of model coefficients, and is a common practice in machine learning."
      ],
      "metadata": {
        "id": "k8NSJMEsG918"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 1)"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer - I chose a test size of 0.2, meaning 20% of the data is reserved for testing and 80% for training. This ratio strikes a balance between having enough data for training and sufficient data for evaluating the model's performance. Additionally, it is a common practice in machine learning and provides statistically significant data for evaluation while reducing overfitting."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer - My dataset is not imbalanced"
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1 Linear Regression (sklearn OLS)"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Initialize Linear Regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Fit the model on the training data\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
        "print(\"Mean Absolute Error (MAE):\", mae)\n",
        "print(\"R-squared (R2):\", r2)\n"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating DataFrames of test and train dataset\n",
        "train_df = pd.DataFrame(x_train,y_train)\n",
        "test_df = pd.DataFrame(y_test)\n",
        "test_df.rename(columns = {'Close':'Actual Closing Price'},inplace = True)"
      ],
      "metadata": {
        "id": "7_Da521QJg3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Predict on the model\n",
        "test_df[' Predicted Closing Price'] = y_pred\n",
        "test_df.head()"
      ],
      "metadata": {
        "id": "JjeUArgFJg0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Actual Price vs. Predicted Price for Linear Regression Plot\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(np.array(y_test))\n",
        "plt.plot(y_pred)\n",
        "plt.suptitle('Actual Vs. Predicted Close Price: Linear Regression', fontsize=16)\n",
        "plt.legend(['Actual','Predicted'], fontsize=12)\n",
        "plt.xlabel('No of Test Data', fontsize=12)\n",
        "plt.ylabel('Closing Price', fontsize=12)\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "sWy_kwO3JrX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ML model used in the provided code snippet is Linear Regression. Linear Regression is a supervised learning algorithm used for predicting a continuous target variable based on one or more input features. It works by finding the best-fitting linear relationship between the input features and the target variable.\n",
        "\n",
        "Evaluation Metric Score Chart:\n",
        "\n",
        "Mean Squared Error (MSE): MSE measures the average squared difference between the actual and predicted values. Lower values indicate better model performance.\n",
        "Root Mean Squared Error (RMSE): RMSE is the square root of MSE, providing a measure of the average magnitude of the errors. It is in the same unit as the target variable.\n",
        "Mean Absolute Error (MAE): MAE measures the average absolute difference between the actual and predicted values. It is less sensitive to outliers compared to MSE.\n",
        "R-squared (R2): R2 measures the proportion of the variance in the target variable that is explained by the model. Higher values indicate better model fit, with 1 indicating a perfect fit."
      ],
      "metadata": {
        "id": "hICx3ilVba4h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Define the Linear Regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Perform cross-validation\n",
        "cv_scores = cross_val_score(model, x_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
        "\n",
        "# Calculate mean squared error (MSE) from cross-validation scores\n",
        "mse = -np.mean(cv_scores)\n",
        "\n",
        "# Calculate root mean squared error (RMSE)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "# Calculate mean absolute error (MAE)\n",
        "mae = -np.mean(cross_val_score(model, x_train, y_train, cv=5, scoring='neg_mean_absolute_error'))\n",
        "\n",
        "# Calculate R-squared (R2)\n",
        "r2 = np.mean(cross_val_score(model, x_train, y_train, cv=5, scoring='r2'))\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
        "print(\"Mean Absolute Error (MAE):\", mae)\n",
        "print(\"R-squared (R2):\", r2)\n"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer - There is mo hyperparameter in sklearn linear regression as its works on OLS."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer - Even after Cross - validation the accuracy drops down slightly from 99.79 to 99.22"
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2 Ridge regression"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Define the Ridge Regression model\n",
        "ridge_model = Ridge(alpha=1.0)\n",
        "\n",
        "# Fit the model on the training data\n",
        "ridge_model.fit(x_train, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred_ridge = ridge_model.predict(x_test)\n",
        "\n",
        "# Calculate Mean Absolute Error (MAE)\n",
        "mae_ridge = mean_absolute_error(y_test, y_pred_ridge)\n",
        "\n",
        "# Calculate Mean Squared Error (MSE)\n",
        "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
        "\n",
        "# Calculate R-squared (R2)\n",
        "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(\"Mean Absolute Error (MAE) - Ridge Regression:\", mae_ridge)\n",
        "print(\"Mean Squared Error (MSE) - Ridge Regression:\", mse_ridge)\n",
        "print(\"R-squared (R2) - Ridge Regression:\", r2_ridge)"
      ],
      "metadata": {
        "id": "tOxcYcXoeSUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Actual Price vs. Predicted Price for Ridge\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(np.array(y_test))\n",
        "plt.plot(y_pred_ridge)\n",
        "plt.suptitle('Actual Vs. Predicted Close Price: Ridge', fontsize=16)\n",
        "plt.legend(['Actual','Predicted'], fontsize=12)\n",
        "plt.xlabel('No of Test Data', fontsize=12)\n",
        "plt.ylabel('Closing Price', fontsize=12)\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import make_scorer, mean_absolute_error, r2_score\n",
        "\n",
        "\n",
        "# Define the Ridge Regression model\n",
        "ridge_model = Ridge()\n",
        "\n",
        "# Define the range of alpha values to tune\n",
        "alpha_values = [0.001, 0.01, 0.1, 1, 10, 100]\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {'alpha': alpha_values}\n",
        "\n",
        "# Define evaluation metrics\n",
        "scoring = {'mae': make_scorer(mean_absolute_error),\n",
        "           'mse': make_scorer(mean_squared_error),\n",
        "           'r2': make_scorer(r2_score)}\n",
        "\n",
        "# Perform grid search with cross-validation\n",
        "grid_search = GridSearchCV(estimator=ridge_model, param_grid=param_grid, cv=5, scoring=scoring, refit='mae')\n",
        "grid_search.fit(x_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_alpha = grid_search.best_params_['alpha']\n",
        "\n",
        "# Get the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Predict on the test data using the best model\n",
        "y_pred_best = best_model.predict(x_test)\n",
        "\n",
        "# Calculate Mean Absolute Error (MAE) using the best model\n",
        "mae_best = mean_absolute_error(y_test, y_pred_best)\n",
        "\n",
        "# Calculate Mean Squared Error (MSE) using the best model\n",
        "mse_best = mean_squared_error(y_test, y_pred_best)\n",
        "\n",
        "# Calculate R-squared (R2) using the best model\n",
        "r2_best = r2_score(y_test, y_pred_best)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(\"Best alpha:\", best_alpha)\n",
        "print(\"Mean Absolute Error (MAE) - Best model:\", mae_best)\n",
        "print(\"Mean Squared Error (MSE) - Best model:\", mse_best)\n",
        "print(\"R-squared (R2) - Best model:\", r2_best)\n"
      ],
      "metadata": {
        "id": "tGg5qHJAg208"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import make_scorer, mean_absolute_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Define the Ridge Regression model\n",
        "ridge_model = Ridge(alpha=100)\n",
        "\n",
        "# Perform cross-validation with Mean Absolute Error (MAE) scoring\n",
        "mae_scores = cross_val_score(ridge_model, x_train, y_train, cv=5, scoring=make_scorer(mean_absolute_error))\n",
        "\n",
        "# Perform cross-validation with R-squared (R2) scoring\n",
        "r2_scores = cross_val_score(ridge_model, x_train, y_train, cv=5, scoring=make_scorer(r2_score))\n",
        "\n",
        "# Calculate the mean scores for MAE and R2\n",
        "mean_mae = np.mean(mae_scores)\n",
        "mean_r2 = np.mean(r2_scores)\n",
        "\n",
        "# Print mean scores\n",
        "print(\"Mean Absolute Error (MAE) - Cross-validation:\", mean_mae)\n",
        "print(\"R-squared (R2) - Cross-validation:\", mean_r2)\n"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer - I chose GridSearchCV for hyperparameter optimization because it offers a comprehensive search over specified parameter grids, integrates cross-validation for reliable performance estimation, automatically selects optimal hyperparameters based on scoring metrics, and provides built-in refitting for the best model. This ensures a systematic and efficient approach to finding the best hyperparameters for the model."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer - From grid search cv most optimum alpha value is 100 as it shows increase in MAE from 3.72 to 19.42 , but r2 score decreased."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - RandomForest"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Define the Random Forest model\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Fit the model on the training data\n",
        "rf_model.fit(x_train, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred_rf = rf_model.predict(x_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "rmse_rf = np.sqrt(mse_rf)\n",
        "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
        "r2_rf = r2_score(y_test, y_pred_rf)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(\"Random Forest Regression Metrics:\")\n",
        "print(\"Mean Squared Error (MSE):\", mse_rf)\n",
        "print(\"Root Mean Squared Error (RMSE):\", rmse_rf)\n",
        "print(\"Mean Absolute Error (MAE):\", mae_rf)\n",
        "print(\"R-squared (R2):\", r2_rf)"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import make_scorer, mean_squared_error, mean_absolute_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Define the Random Forest model\n",
        "rf_model = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],  # Number of trees in the forest\n",
        "    'max_depth': [3, 5, 10, 15],   # Maximum depth of the trees\n",
        "    'min_samples_split': [2, 5, 10],   # Minimum number of samples required to split a node\n",
        "    'min_samples_leaf': [1, 2, 4]      # Minimum number of samples required at each leaf node\n",
        "}\n",
        "\n",
        "# Define evaluation metrics\n",
        "scoring = {'mse': make_scorer(mean_squared_error),\n",
        "           'mae': make_scorer(mean_absolute_error),\n",
        "           'r2': make_scorer(r2_score)}\n",
        "\n",
        "# Perform grid search with cross-validation\n",
        "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, scoring=scoring, refit='mae')\n",
        "grid_search.fit(x_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# Get the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Predict on the test data using the best model\n",
        "y_pred_best = best_model.predict(x_test)\n",
        "\n",
        "# Calculate evaluation metrics using the best model\n",
        "mse_best = mean_squared_error(y_test, y_pred_best)\n",
        "mae_best = mean_absolute_error(y_test, y_pred_best)\n",
        "r2_best = r2_score(y_test, y_pred_best)\n",
        "\n",
        "# Print the best hyperparameters and evaluation metrics\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "print(\"Mean Squared Error (MSE) - Best model:\", mse_best)\n",
        "print(\"Mean Absolute Error (MAE) - Best model:\", mae_best)\n",
        "print(\"R-squared (R2) - Best model:\", r2_best)\n"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import make_scorer, mean_squared_error, mean_absolute_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Define the Random Forest model with specified hyperparameters\n",
        "rf_model = RandomForestRegressor(n_estimators=200, max_depth=3, min_samples_split=10, min_samples_leaf=1, random_state=42)\n",
        "\n",
        "# Define evaluation metrics\n",
        "scoring = {'mse': make_scorer(mean_squared_error),\n",
        "           'mae': make_scorer(mean_absolute_error),\n",
        "           'r2': make_scorer(r2_score)}\n",
        "\n",
        "# Perform cross-validation with specified hyperparameters\n",
        "mse_scores = cross_val_score(rf_model, x_train, y_train, cv=5, scoring=scoring['mse'])\n",
        "mae_scores = cross_val_score(rf_model, x_train, y_train, cv=5, scoring=scoring['mae'])\n",
        "r2_scores = cross_val_score(rf_model, x_train, y_train, cv=5, scoring=scoring['r2'])\n",
        "\n",
        "# Calculate mean scores for evaluation metrics\n",
        "mean_mse = np.mean(mse_scores)\n",
        "mean_mae = np.mean(mae_scores)\n",
        "mean_r2 = np.mean(r2_scores)\n",
        "\n",
        "# Print mean scores\n",
        "print(\"Cross-validation results for specified hyperparameters:\")\n",
        "print(\"Mean Squared Error (MSE):\", mean_mse)\n",
        "print(\"Mean Absolute Error (MAE):\", mean_mae)\n",
        "print(\"R-squared (R2):\", mean_r2)\n"
      ],
      "metadata": {
        "id": "_YLBqnsK35oy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grid Search Cross-Validation (GridSearchCV) was used for hyperparameter optimization. It systematically explores a predefined grid of hyperparameter values, integrates cross-validation for reliable performance estimation, and selects the best hyperparameters that minimize a specified metric, such as Mean Absolute Error (MAE). This technique was chosen for its comprehensive search, controlled search space, and effectiveness in improving model performance."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer - Using R-squared (R2) score instead of Mean Squared Error (MSE) when outliers are present is a prudent choice. R2 is less affected by outliers, providing a more robust evaluation of the model's performance. This approach prioritizes a metric that accurately reflects the model's ability to explain the variance in the data, considering the potential influence of outliers."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer -"
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the evaluation metrics for each model\n",
        "metrics_data = {\n",
        "    'Model': ['Linear Regression', 'Ridge Regression', 'Random Forest'],\n",
        "    'MSE': [18.69, 29.01, 35.29],\n",
        "    'MAE': [3.72, 3.72, 4.20],\n",
        "    'R2': [0.997, 0.997, 0.996]\n",
        "}\n",
        "\n",
        "# Create a DataFrame from the metrics data\n",
        "metrics_df = pd.DataFrame(metrics_data)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(\"Performance Comparison of Different Models:\")\n",
        "print(metrics_df)\n"
      ],
      "metadata": {
        "id": "7qRpTbOYUjAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer - Selecting linear regression as the final model based on its superior performance is a valid decision. Linear regression's simplicity and interpretability make it an attractive choice, especially if it achieves satisfactory performance compared to more complex models. By prioritizing the model with the best performance metrics, you're ensuring that your final model accurately captures the relationship between the features and the target variable, leading to more reliable predictions and potentially better business outcomes."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The exploration and refinement journey of the Yes Bank stock price dataset have been meticulous, ensuring a robust foundation for modeling.\n",
        "\n",
        "Throughout the process:\n",
        "\n",
        "1. **Data Integrity**: The dataset stood out with its completeness, devoid of null values and outliers, laying a solid groundwork for analysis.\n",
        "\n",
        "2. **Insightful Visualizations**: Skewed features underwent log transformation, unveiling hidden patterns while ensuring normalization. Notably, a strong positive linear correlation emerged among variables, underscoring their interdependencies.\n",
        "\n",
        "3. **Detecting Trends**: Astute observations uncovered intriguing trends, such as the post-2017 decline in stock value and the remarkable surge within a confined 10-month window in 2014.\n",
        "\n",
        "4. **Multicollinearity**: Despite encountering high multicollinearity, no feature engineering was pursued, as extremely large VIFs indicated the equal significance of all variables.\n",
        "\n",
        "5. **Normalization and Scaling**: Leveraging StandardScaler ensured uniform scaling across features, refining the dataset for modeling.\n",
        "\n",
        "6. **Hypothesis Testing**: Rigorous hypothesis testing shed light on crucial insights, affirming the historical mean closing price equality and the non-stationarity of stock closing prices.\n",
        "\n",
        "7. **Model Selection**: A comprehensive array of regression models, including Linear Regression and Random Forest Regressor, underwent rigorous evaluation. Ultimately, the Hyperparameter-tuned Linear Regressor emerged triumphant, boasting superior performance.\n",
        "\n",
        "8. **Feature Importance**: Delving deeper, Permutation Importance Scores highlighted the pivotal role of the \"Low\" feature in predicting the target variable, closely followed by the \"High\" feature. Conversely, \"Open,\" \"Year,\" \"Month,\" and \"Quarter\" features exhibited relatively lower importance, with \"Open\" ranking as the least significant.\n",
        "\n",
        "In essence, the journey from data exploration to model selection has been a testament to thoroughness and precision, culminating in the selection of the Hyperparameter-tuned Linear Regressor as the beacon of predictive prowess."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}